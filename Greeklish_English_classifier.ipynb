{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Importing Libraries"
      ],
      "metadata": {
        "id": "wdK15MUPq7Le"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc1LvZ-0eNUw",
        "outputId": "5ba4bbdc-0a78-474a-ffad-c111d18b17ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Collecting scrapy\n",
            "  Downloading Scrapy-2.12.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.30.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.0)\n",
            "Collecting Twisted>=21.7.0 (from scrapy)\n",
            "  Downloading twisted-24.11.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: cryptography>=37.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (43.0.3)\n",
            "Collecting cssselect>=0.9.1 (from scrapy)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting itemloaders>=1.0.1 (from scrapy)\n",
            "  Downloading itemloaders-1.3.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting parsel>=1.5.0 (from scrapy)\n",
            "  Downloading parsel-1.10.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyOpenSSL>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2.1)\n",
            "Collecting queuelib>=1.4.2 (from scrapy)\n",
            "  Downloading queuelib-1.8.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting service-identity>=18.1.0 (from scrapy)\n",
            "  Downloading service_identity-24.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting w3lib>=1.17.0 (from scrapy)\n",
            "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting zope.interface>=5.1.0 (from scrapy)\n",
            "  Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protego>=0.1.15 (from scrapy)\n",
            "  Downloading Protego-0.4.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting itemadapter>=0.1.0 (from scrapy)\n",
            "  Downloading itemadapter-0.11.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2)\n",
            "Collecting tldextract (from scrapy)\n",
            "  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: lxml>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (5.3.1)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from scrapy) (0.7.1)\n",
            "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
            "  Downloading PyDispatcher-2.0.7-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.29.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (11.1.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.2)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (3.9.1)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=37.0.0->scrapy) (1.17.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (25.3.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.2)\n",
            "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (3.18.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting automat>=24.8.0 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading Automat-24.8.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting constantly>=15.1 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting hyperlink>=17.1.1 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting incremental>=24.7.0 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading incremental-24.7.2-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from zope.interface>=5.1.0->scrapy) (75.2.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=37.0.0->scrapy) (2.22)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Downloading Scrapy-2.12.0-py2.py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading selenium-4.30.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading itemadapter-0.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading itemloaders-1.3.2-py3-none-any.whl (12 kB)\n",
            "Downloading parsel-1.10.0-py2.py3-none-any.whl (17 kB)\n",
            "Downloading Protego-0.4.0-py2.py3-none-any.whl (8.6 kB)\n",
            "Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
            "Downloading queuelib-1.8.0-py3-none-any.whl (13 kB)\n",
            "Downloading service_identity-24.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.29.0-py3-none-any.whl (492 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading twisted-24.11.0-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
            "Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.8/259.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Automat-24.8.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
            "Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading incremental-24.7.2-py3-none-any.whl (20 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13540 sha256=2d68bbf937ebd5f331fd5fc2a814bb0a040ea5fbdb30727acf8bfb40c7b3a05d\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/f8/cce3a9ae6d828bd346be695f7ff54612cd22b7cbd7208d68f3\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3341 sha256=0ae6d69dd2fb94d2efa76c13535e5c224d2d57315dac2117eb3130821e2fbad9\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/d5/72/9cd9eccc819636436c6a6e59c22a0fb1ec167beef141f56491\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398380 sha256=bc2638b370da88f9163a151ea4539db846f77ddaee00845dadcd15cc5bd51c3c\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/a1/46/8e68055c1713f9c4598774c15ad0541f26d5425ee7423b6493\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=bd261432fc91fa0e5e05586f2c5ce6f37fa13f9ba34e8fe4422f71ddd07a8979\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, PyDispatcher, jieba3k, zope.interface, wsproto, w3lib, queuelib, protego, outcome, jmespath, itemadapter, incremental, hyperlink, feedparser, cssselect, constantly, automat, Twisted, trio, requests-file, parsel, feedfinder2, trio-websocket, tldextract, service-identity, itemloaders, selenium, scrapy, newspaper3k\n",
            "Successfully installed PyDispatcher-2.0.7 Twisted-24.11.0 automat-24.8.1 constantly-23.10.4 cssselect-1.3.0 feedfinder2-0.0.4 feedparser-6.0.11 hyperlink-21.0.0 incremental-24.7.2 itemadapter-0.11.0 itemloaders-1.3.2 jieba3k-0.35.1 jmespath-1.0.1 newspaper3k-0.2.8 outcome-1.3.0.post0 parsel-1.10.0 protego-0.4.0 queuelib-1.8.0 requests-file-2.1.0 scrapy-2.12.0 selenium-4.30.0 service-identity-24.2.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.1.3 trio-0.29.0 trio-websocket-0.12.2 w3lib-2.3.1 wsproto-1.2.0 zope.interface-7.2\n"
          ]
        }
      ],
      "source": [
        "pip install requests beautifulsoup4 scrapy selenium newspaper3k\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting Greek data"
      ],
      "metadata": {
        "id": "nOKMCSaDrY1s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Resource 1"
      ],
      "metadata": {
        "id": "Jkl8xdM6rpkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# List of Greeklish forum URLs\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# List of Greeklish forum URLs\n",
        "urls = [\n",
        "    \"https://www.textkit.com/greek-latin-forum/viewtopic.php?t=64754\",\n",
        "    \"https://www.prestashop.com/forums/topic/1046273-xml-%CF%80%CF%81%CE%BF%CE%B9%CE%BF%CE%BD%CF%84%CE%B1/\",\n",
        "    \"https://athinapoli.gr/category/city/\",\n",
        "    \"https://greek-articles.blogspot.com/?utm_source=chatgpt.com\",\n",
        "    \"\"\n",
        "]\n",
        "\n",
        "# Open file to save data\n",
        "with open(\"greeklish_data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for url in urls:\n",
        "        print(f\"Scraping: {url}\")\n",
        "        try:\n",
        "            # Get webpage content\n",
        "            response = requests.get(url, timeout=10)\n",
        "            response.raise_for_status()  # Raise error for bad response\n",
        "\n",
        "            # Parse content\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "            # Extract text from paragraphs\n",
        "            texts = [p.text.strip() for p in soup.find_all(\"p\") if p.text.strip()]\n",
        "\n",
        "            # Save extracted text\n",
        "            for text in texts:\n",
        "                f.write(text + \"\\n\")\n",
        "\n",
        "            print(f\"✅ Data saved from {url}\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"❌ Failed to scrape {url}: {e}\")\n",
        "\n",
        "print(\"✅ Greeklish data collection completed!\")\n",
        "\n",
        "# Open file to save data\n",
        "with open(\"greeklish_data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for url in urls:\n",
        "        print(f\"Scraping: {url}\")\n",
        "        try:\n",
        "            # Get webpage content\n",
        "            response = requests.get(url, timeout=10)\n",
        "            response.raise_for_status()  # Raise error for bad response\n",
        "\n",
        "            # Parse content\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "            # Extract text from paragraphs\n",
        "            texts = [p.text.strip() for p in soup.find_all(\"p\") if p.text.strip()]\n",
        "\n",
        "            # Save extracted text\n",
        "            for text in texts:\n",
        "                f.write(text + \"\\n\")\n",
        "\n",
        "            print(f\"✅ Data saved from {url}\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"❌ Failed to scrape {url}: {e}\")\n",
        "\n",
        "print(\"✅ Greeklish data collection completed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qm6jOSUhqa0F",
        "outputId": "5e53cea2-848d-4348-f864-d610015efc95"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: https://www.textkit.com/greek-latin-forum/viewtopic.php?t=64754\n",
            "✅ Data saved from https://www.textkit.com/greek-latin-forum/viewtopic.php?t=64754\n",
            "Scraping: https://www.prestashop.com/forums/topic/1046273-xml-%CF%80%CF%81%CE%BF%CE%B9%CE%BF%CE%BD%CF%84%CE%B1/\n",
            "✅ Data saved from https://www.prestashop.com/forums/topic/1046273-xml-%CF%80%CF%81%CE%BF%CE%B9%CE%BF%CE%BD%CF%84%CE%B1/\n",
            "Scraping: https://athinapoli.gr/category/city/\n",
            "✅ Data saved from https://athinapoli.gr/category/city/\n",
            "✅ Greeklish data collection completed!\n",
            "Scraping: https://www.textkit.com/greek-latin-forum/viewtopic.php?t=64754\n",
            "✅ Data saved from https://www.textkit.com/greek-latin-forum/viewtopic.php?t=64754\n",
            "Scraping: https://www.prestashop.com/forums/topic/1046273-xml-%CF%80%CF%81%CE%BF%CE%B9%CE%BF%CE%BD%CF%84%CE%B1/\n",
            "✅ Data saved from https://www.prestashop.com/forums/topic/1046273-xml-%CF%80%CF%81%CE%BF%CE%B9%CE%BF%CE%BD%CF%84%CE%B1/\n",
            "Scraping: https://athinapoli.gr/category/city/\n",
            "✅ Data saved from https://athinapoli.gr/category/city/\n",
            "✅ Greeklish data collection completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Resource 2"
      ],
      "metadata": {
        "id": "zpVnHmJtrwmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# List of Greeklish forum URLs\n",
        "urls = [\n",
        "    \"https://www.greek-chat.gr/oroi-xrisis.html\",\n",
        "    \"https://www.textkit.com/greek-latin-forum/viewtopic.php?t=64754\",\n",
        "    \"https://www.example.com/greeklish-thread\"  # Add more URLs\n",
        "]\n",
        "\n",
        "# Open file to save data\n",
        "with open(\"greeklish_data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for url in urls:\n",
        "        print(f\"🔍 Scraping: {url}\")\n",
        "        try:\n",
        "            # Get webpage content\n",
        "            response = requests.get(url, timeout=10)\n",
        "            response.raise_for_status()  # Raise error for bad response\n",
        "\n",
        "            # Parse content\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "            # Extract text from paragraphs\n",
        "            texts = [p.text.strip() for p in soup.find_all(\"p\") if p.text.strip()]\n",
        "\n",
        "            # Save extracted text\n",
        "            for text in texts:\n",
        "                f.write(text + \"\\n\")\n",
        "\n",
        "            print(f\"✅ Data saved from {url}\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"❌ Failed to scrape {url}: {e}\")\n",
        "\n",
        "print(\"✅ Greeklish data collection completed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qd77GtnrQWc",
        "outputId": "ffcfae3e-d4cf-4217-d96e-35db5525f08d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greeklish data saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install praw\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJK-bIT8uZ8e",
        "outputId": "1a61ce0e-1a23-4d73-e456-bbfab3cb0999"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: praw in /usr/local/lib/python3.11/dist-packages (7.8.1)\n",
            "Requirement already satisfied: prawcore<3,>=2.4 in /usr/local/lib/python3.11/dist-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update_checker>=0.18 in /usr/local/lib/python3.11/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import time\n",
        "\n",
        "# Reddit API Credentials (Replace with your credentials)\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"Gh9DC2beMVkbPSQDO6VJ0Q\",\n",
        "    client_secret=\"ZHMAjJaWDvYgjmbeoRShXxAZqHnO_g\",\n",
        "    user_agent=\"GreeklishScraper\"\n",
        ")\n",
        "\n",
        "# List of subreddits to scrape\n",
        "subreddits = [\"greekstories\", \"Greek\", \"learn_greek\" , \"GreekCulture\"]   # Add more subreddits\n",
        "\n",
        "# List to store scraped data\n",
        "posts = []\n",
        "\n",
        "try:\n",
        "    for sub in subreddits:\n",
        "        print(f\"🔍 Scraping r/{sub}...\")\n",
        "        subreddit = reddit.subreddit(sub)\n",
        "\n",
        "        for post in subreddit.hot(limit=10):  # Adjust limit as needed\n",
        "            post_data = []\n",
        "\n",
        "            # Store post title and content\n",
        "            post_data.append(f\"🔹 Title: {post.title}\")\n",
        "            post_data.append(f\"📝 Content: {post.selftext.strip() if post.selftext else '[No Content]'}\")\n",
        "\n",
        "            # Get top-level comments\n",
        "            post.comments.replace_more(limit=0)  # Load all comments\n",
        "            top_comments = [comment.body.strip() for comment in post.comments[:10] if comment.body.strip()]\n",
        "\n",
        "            # Ensure posts without comments are still saved\n",
        "            if top_comments:\n",
        "                post_data.append(\"💬 Top Comments:\")\n",
        "                post_data.extend(top_comments)\n",
        "            else:\n",
        "                post_data.append(\"💬 No comments available.\")\n",
        "\n",
        "            # Add separator and store the data\n",
        "            post_data.append(\"=\" * 50)\n",
        "            posts.append(\"\\n\".join(post_data))\n",
        "\n",
        "            time.sleep(2)  # Prevent rate limiting\n",
        "\n",
        "    # Save to a text file\n",
        "    with open(\"reddit_greeklish_data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\\n\".join(posts))\n",
        "\n",
        "    print(\"✅ Reddit Greeklish data saved successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "ZgEBA8UK8ALe",
        "outputId": "57b42eeb-180b-4358-b52e-efa32909ab7d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'praw'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-be76718dfafa>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpraw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Reddit API Credentials (Replace with your credentials)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m reddit = praw.Reddit(\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'praw'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting English data"
      ],
      "metadata": {
        "id": "x3VSbDwJBMit"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resource 1\n"
      ],
      "metadata": {
        "id": "AzPr29VJBRad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade lxml_html_clean newspaper3k\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeBkHvcQCn6D",
        "outputId": "ccfc8e69-dc75-4c7d-d8fd-33b1ab42379e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.11/dist-packages (0.2.8)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from lxml_html_clean) (5.3.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (4.13.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (11.1.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.2)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (1.3.0)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (3.9.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.32.3)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.11)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.1.3)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.8.2)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.13.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.11/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2025.1.31)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.18.0)\n",
            "Downloading lxml_html_clean-0.4.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: lxml_html_clean\n",
            "Successfully installed lxml_html_clean-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from newspaper import Article\n",
        "\n",
        "# List of news URLs (Add more as needed)\n",
        "news_urls = [\n",
        "    \"https://www.bbc.com/news/articles/c807lm2003zo\",\n",
        "    \"https://edition.cnn.com/2025/03/31/middleeast/aid-workers-found-gaza-mass-grave-intl-hnk/index.html\",\n",
        "    \"https://www.theguardian.com/world/2025/apr/03/canada-trump-tariffs-exemption\",\n",
        "    \"\",\n",
        "    \"\"\n",
        "]\n",
        "\n",
        "english_sentences = []\n",
        "\n",
        "for url in news_urls:\n",
        "    try:\n",
        "        article = Article(url)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "\n",
        "        if article.text.strip():  # Ensure there's text\n",
        "            english_sentences.append(article.text.strip())\n",
        "            print(f\"✅ Extracted: {url}\")\n",
        "        else:\n",
        "            print(f\"⚠️ No text found: {url}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing {url}: {e}\")\n",
        "\n",
        "# Save extracted English data\n",
        "with open(\"english_data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\\n\".join(english_sentences))\n",
        "\n",
        "print(\"✅ English news data saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2mCEo4-BtoE",
        "outputId": "53d6898e-b545-4d53-9a50-fa2d10109b0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Extracted: https://www.bbc.com/news/articles/c807lm2003zo\n",
            "✅ Extracted: https://edition.cnn.com/2025/03/31/middleeast/aid-workers-found-gaza-mass-grave-intl-hnk/index.html\n",
            "✅ Extracted: https://www.theguardian.com/world/2025/apr/03/canada-trump-tariffs-exemption\n",
            "✅ English news data saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **🔹 Data Cleaning & CSV Generation for Greeklish vs. English**  \n",
        "\n",
        "### **📌 Steps**  \n",
        "\n",
        "1️⃣ **Text Cleaning (`clean_text`)**  \n",
        "✔ Converts text to **lowercase**  \n",
        "✔ Removes **URLs & special characters** (keeps Greek & English)  \n",
        "✔ Strips **extra spaces** for consistency  \n",
        "\n",
        "2️⃣ **Load & Preprocess Data**  \n",
        "✔ Reads **Greeklish & English** text files  \n",
        "✔ Removes **short sentences (≤4 words)**  \n",
        "\n",
        "3️⃣ **Save as CSV (`dataset.csv`)**  \n",
        "✔ Creates a **structured dataset** for classification  \n",
        "✔ Stores **labeled text** for model training  \n",
        "\n"
      ],
      "metadata": {
        "id": "RInCMwMfKFpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Lowercase\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n",
        "    text = re.sub(r\"[^a-zA-Zα-ωΑ-Ω\\s]\", \"\", text)  # Remove special characters (allow Greek chars)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "# Load data from text files\n",
        "with open(\"greeklish_data1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    greeklish_sentences = [clean_text(line.strip()) for line in f if len(line.split()) > 4]\n",
        "\n",
        "with open(\"greeklish_data2.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    greeklish_sentences = [clean_text(line.strip()) for line in f if len(line.split()) > 4]\n",
        "\n",
        "with open(\"reddit_greeklish_data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    greeklish_sentences = [clean_text(line.strip()) for line in f if len(line.split()) > 4]\n",
        "\n",
        "\n",
        "with open(\"english_data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    english_sentences = [clean_text(line.strip()) for line in f if len(line.split()) > 4]\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    \"Sentence\": greeklish_sentences + english_sentences,\n",
        "    \"Label\": [\"Greeklish\"] * len(greeklish_sentences) + [\"English\"] * len(english_sentences)\n",
        "})\n",
        "\n",
        "# Save as CSV\n",
        "df.to_csv(\"dataset.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"✅ Dataset saved successfully as dataset.csv!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvZoO9iLDfOK",
        "outputId": "b8321ff4-9220-4bcf-e280-c48467e9fd10"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset saved successfully as dataset.csv!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **🔹 Data Cleaning & Preprocessing for Greeklish vs. English Classification**  \n",
        "\n",
        "### **📌 Steps**  \n",
        "\n",
        "1️⃣ **Text Cleaning (`clean_text`)**  \n",
        "✔ Convert to **lowercase** & remove **extra spaces**  \n",
        "✔ Remove **URLs & special characters** (keep English & Greek)  \n",
        "✔ Normalize **repeated characters** (`helloooo → helloo`)  \n",
        "\n",
        "2️⃣ **Auto-Label Validation (`validate_language`)**  \n",
        "✔ Detects **incorrectly classified sentences** using `langdetect`  \n",
        "✔ Removes misclassified **Greeklish & English texts**  \n",
        "\n",
        "3️⃣ **Filter Short Sentences**  \n",
        "✔ Removes **sentences with ≤4 words** for better data quality  \n",
        "\n",
        "4️⃣ **Save Cleaned Data**  \n",
        "✔ Stores structured **CSV (`dataset_cleaned.csv`)** for model training  \n",
        "\n"
      ],
      "metadata": {
        "id": "Sun7A02lJn0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "k25SoxJXKJ6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mNf3O6JJTlX",
        "outputId": "b1bcffe4-5fbc-4e94-bf2d-6be52562b1ec"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from langdetect import detect\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    text = text.lower().strip()  # Convert to lowercase and remove leading/trailing spaces\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n",
        "    text = re.sub(r\"[^a-zA-Zα-ωΑ-Ω\\s]\", \"\", text)  # Keep only English & Greek characters\n",
        "    text = re.sub(r\"\\s+\", \" \", text)  # Normalize multiple spaces\n",
        "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)  # Limit repeated characters (e.g., helloooo → helloo)\n",
        "    return text\n",
        "\n",
        "# Function to validate if sentence matches expected language label\n",
        "def validate_language(sentence, expected_label):\n",
        "    try:\n",
        "        detected_lang = detect(sentence)  # Detect language\n",
        "        if expected_label == \"Greeklish\" and detected_lang == \"en\":\n",
        "            return False  # Misclassified as English\n",
        "        if expected_label == \"English\" and detected_lang != \"en\":\n",
        "            return False  # Misclassified as Greeklish\n",
        "    except:\n",
        "        return False  # If detection fails, discard sentence\n",
        "    return True\n",
        "\n",
        "# Load and clean Greeklish data\n",
        "with open(\"greeklish_data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    greeklish_sentences = [clean_text(line.strip()) for line in f if len(line.split()) > 4]\n",
        "\n",
        "# Load and clean English data\n",
        "with open(\"english_data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    english_sentences = [clean_text(line.strip()) for line in f if len(line.split()) > 4]\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame({\n",
        "    \"Sentence\": greeklish_sentences + english_sentences,\n",
        "    \"Label\": [\"Greeklish\"] * len(greeklish_sentences) + [\"English\"] * len(english_sentences)\n",
        "})\n",
        "\n",
        "# Filter out sentences with incorrect labels\n",
        "df = df[df.apply(lambda row: validate_language(row[\"Sentence\"], row[\"Label\"]), axis=1)]\n",
        "\n",
        "# Save the cleaned dataset as a CSV file\n",
        "df.to_csv(\"dataset_cleaned.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"✅ Cleaned dataset saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VItCj0mfJLFA",
        "outputId": "4eb363d9-2941-49f2-a362-4f5ef8c29ed4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Cleaned dataset saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)  # ✅ Correct\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ywg6sXHgf_B",
        "outputId": "78cde1f4-a1f9-4e07-fc90-b57843be32f9"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(159, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib\n",
        "\n",
        "# Load cleaned dataset\n",
        "df = pd.read_csv(\"dataset_cleaned.csv\")\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)  # Unigrams & Bigrams\n",
        "X = vectorizer.fit_transform(df[\"Sentence\"])  # Convert text to numerical form\n",
        "y = df[\"Label\"]  # Target labels\n",
        "\n",
        "# Split into Training & Testing Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression Classifier\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Model Evaluation\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"✅ Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Show Classification Report\n",
        "print(\"\\n🔹 Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Save Model & Vectorizer for Future Use\n",
        "joblib.dump(classifier, \"text_classifier.pkl\")\n",
        "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
        "\n",
        "print(\"✅ Model & Vectorizer saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pg39GFSUKNa_",
        "outputId": "7aa72093-f01d-4c28-815e-cc269ed7bbce"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model Accuracy: 1.0000\n",
            "\n",
            "🔹 Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     English       1.00      1.00      1.00        15\n",
            "   Greeklish       1.00      1.00      1.00        17\n",
            "\n",
            "    accuracy                           1.00        32\n",
            "   macro avg       1.00      1.00      1.00        32\n",
            "weighted avg       1.00      1.00      1.00        32\n",
            "\n",
            "✅ Model & Vectorizer saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"dataset_cleaned.csv\")\n",
        "\n",
        "# Convert labels to numbers (Greeklish = 1, English = 0)\n",
        "df[\"Label\"] = df[\"Label\"].map({\"Greeklish\": 1, \"English\": 0})\n",
        "\n",
        "# Tokenization (Convert words into numerical values)\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(df[\"Sentence\"])\n",
        "X = tokenizer.texts_to_sequences(df[\"Sentence\"])\n",
        "\n",
        "# Padding sequences (ensures all inputs have the same length)\n",
        "X = pad_sequences(X, maxlen=50, padding=\"post\", truncating=\"post\")\n",
        "y = np.array(df[\"Label\"])\n",
        "\n",
        "# Split data (80% Train, 20% Test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build LSTM Model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=5000, output_dim=128, input_length=50),\n",
        "    LSTM(128, return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    LSTM(64),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation=\"sigmoid\")  # Sigmoid for binary classification\n",
        "])\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train Model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Save Model & Tokenizer\n",
        "model.save(\"lstm_text_classifier.h5\")\n",
        "joblib.dump(tokenizer, \"tokenizer.pkl\")\n",
        "\n",
        "print(\"✅ Model & Tokenizer saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dd4eRUnY9Iz",
        "outputId": "6c7feb9d-7003-4236-83bb-960869df49cd"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 332ms/step - accuracy: 0.4402 - loss: 0.6945 - val_accuracy: 0.6250 - val_loss: 0.6815\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step - accuracy: 0.6561 - loss: 0.6786 - val_accuracy: 0.6562 - val_loss: 0.6480\n",
            "Epoch 3/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step - accuracy: 0.6480 - loss: 0.6202 - val_accuracy: 0.8750 - val_loss: 0.4555\n",
            "Epoch 4/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 212ms/step - accuracy: 0.9290 - loss: 0.3541 - val_accuracy: 0.8750 - val_loss: 0.3032\n",
            "Epoch 5/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 288ms/step - accuracy: 0.9770 - loss: 0.1302 - val_accuracy: 1.0000 - val_loss: 0.0339\n",
            "Epoch 6/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 270ms/step - accuracy: 0.9916 - loss: 0.0616 - val_accuracy: 1.0000 - val_loss: 0.0149\n",
            "Epoch 7/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step - accuracy: 0.9854 - loss: 0.0742 - val_accuracy: 1.0000 - val_loss: 0.0098\n",
            "Epoch 8/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step - accuracy: 0.9948 - loss: 0.0342 - val_accuracy: 1.0000 - val_loss: 0.0075\n",
            "Epoch 9/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 177ms/step - accuracy: 0.9916 - loss: 0.0449 - val_accuracy: 1.0000 - val_loss: 0.0069\n",
            "Epoch 10/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step - accuracy: 0.9969 - loss: 0.0224 - val_accuracy: 1.0000 - val_loss: 0.0065\n",
            "Epoch 11/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step - accuracy: 0.9854 - loss: 0.0689 - val_accuracy: 1.0000 - val_loss: 0.0072\n",
            "Epoch 12/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step - accuracy: 0.9916 - loss: 0.0485 - val_accuracy: 1.0000 - val_loss: 0.0078\n",
            "Epoch 13/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step - accuracy: 0.9948 - loss: 0.0289 - val_accuracy: 1.0000 - val_loss: 0.0079\n",
            "Epoch 14/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step - accuracy: 0.9969 - loss: 0.0245 - val_accuracy: 1.0000 - val_loss: 0.0075\n",
            "Epoch 15/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - accuracy: 0.9948 - loss: 0.0308 - val_accuracy: 1.0000 - val_loss: 0.0077\n",
            "Epoch 16/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 169ms/step - accuracy: 0.9854 - loss: 0.0635 - val_accuracy: 1.0000 - val_loss: 0.0082\n",
            "Epoch 17/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step - accuracy: 0.9969 - loss: 0.0239 - val_accuracy: 1.0000 - val_loss: 0.0076\n",
            "Epoch 18/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step - accuracy: 0.9948 - loss: 0.0316 - val_accuracy: 1.0000 - val_loss: 0.0076\n",
            "Epoch 19/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step - accuracy: 0.9916 - loss: 0.0472 - val_accuracy: 1.0000 - val_loss: 0.0081\n",
            "Epoch 20/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 311ms/step - accuracy: 0.9916 - loss: 0.0425 - val_accuracy: 1.0000 - val_loss: 0.0082\n",
            "Epoch 21/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 279ms/step - accuracy: 0.9916 - loss: 0.0469 - val_accuracy: 1.0000 - val_loss: 0.0082\n",
            "Epoch 22/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 266ms/step - accuracy: 0.9854 - loss: 0.0688 - val_accuracy: 1.0000 - val_loss: 0.0084\n",
            "Epoch 23/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - accuracy: 0.9948 - loss: 0.0324 - val_accuracy: 1.0000 - val_loss: 0.0077\n",
            "Epoch 24/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 164ms/step - accuracy: 0.9916 - loss: 0.0439 - val_accuracy: 1.0000 - val_loss: 0.0074\n",
            "Epoch 25/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 152ms/step - accuracy: 0.9916 - loss: 0.0460 - val_accuracy: 1.0000 - val_loss: 0.0071\n",
            "Epoch 26/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step - accuracy: 0.9948 - loss: 0.0314 - val_accuracy: 1.0000 - val_loss: 0.0068\n",
            "Epoch 27/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step - accuracy: 0.9916 - loss: 0.0461 - val_accuracy: 1.0000 - val_loss: 0.0068\n",
            "Epoch 28/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step - accuracy: 0.9916 - loss: 0.0397 - val_accuracy: 1.0000 - val_loss: 0.0070\n",
            "Epoch 29/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 165ms/step - accuracy: 0.9854 - loss: 0.0777 - val_accuracy: 1.0000 - val_loss: 0.0074\n",
            "Epoch 30/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step - accuracy: 0.9916 - loss: 0.0510 - val_accuracy: 1.0000 - val_loss: 0.0077\n",
            "Epoch 31/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 167ms/step - accuracy: 0.9969 - loss: 0.0227 - val_accuracy: 1.0000 - val_loss: 0.0072\n",
            "Epoch 32/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step - accuracy: 0.9916 - loss: 0.0429 - val_accuracy: 1.0000 - val_loss: 0.0075\n",
            "Epoch 33/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - accuracy: 0.9916 - loss: 0.0404 - val_accuracy: 1.0000 - val_loss: 0.0076\n",
            "Epoch 34/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step - accuracy: 0.9916 - loss: 0.0393 - val_accuracy: 1.0000 - val_loss: 0.0074\n",
            "Epoch 35/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 193ms/step - accuracy: 0.9854 - loss: 0.0709 - val_accuracy: 1.0000 - val_loss: 0.0075\n",
            "Epoch 36/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 270ms/step - accuracy: 0.9969 - loss: 0.0218 - val_accuracy: 1.0000 - val_loss: 0.0068\n",
            "Epoch 37/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 380ms/step - accuracy: 0.9854 - loss: 0.0610 - val_accuracy: 1.0000 - val_loss: 0.0070\n",
            "Epoch 38/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 330ms/step - accuracy: 0.9969 - loss: 0.0210 - val_accuracy: 1.0000 - val_loss: 0.0065\n",
            "Epoch 39/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 325ms/step - accuracy: 0.9969 - loss: 0.0226 - val_accuracy: 1.0000 - val_loss: 0.0063\n",
            "Epoch 40/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 294ms/step - accuracy: 0.9854 - loss: 0.0728 - val_accuracy: 1.0000 - val_loss: 0.0071\n",
            "Epoch 41/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 270ms/step - accuracy: 0.9969 - loss: 0.0216 - val_accuracy: 1.0000 - val_loss: 0.0070\n",
            "Epoch 42/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step - accuracy: 0.9854 - loss: 0.0774 - val_accuracy: 1.0000 - val_loss: 0.0076\n",
            "Epoch 43/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - accuracy: 0.9854 - loss: 0.0701 - val_accuracy: 1.0000 - val_loss: 0.0080\n",
            "Epoch 44/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step - accuracy: 0.9948 - loss: 0.0343 - val_accuracy: 1.0000 - val_loss: 0.0076\n",
            "Epoch 45/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step - accuracy: 0.9854 - loss: 0.0625 - val_accuracy: 1.0000 - val_loss: 0.0077\n",
            "Epoch 46/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step - accuracy: 0.9854 - loss: 0.0705 - val_accuracy: 1.0000 - val_loss: 0.0077\n",
            "Epoch 47/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step - accuracy: 0.9916 - loss: 0.0406 - val_accuracy: 1.0000 - val_loss: 0.0073\n",
            "Epoch 48/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step - accuracy: 0.9916 - loss: 0.0446 - val_accuracy: 1.0000 - val_loss: 0.0071\n",
            "Epoch 49/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 177ms/step - accuracy: 0.9916 - loss: 0.0461 - val_accuracy: 1.0000 - val_loss: 0.0070\n",
            "Epoch 50/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step - accuracy: 0.9969 - loss: 0.0231 - val_accuracy: 1.0000 - val_loss: 0.0064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model & Tokenizer saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Load the trained Logistic Regression model\n",
        "logistic_model = joblib.load(\"text_classifier.pkl\")  # Make sure you saved this\n",
        "vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")  # TF-IDF vectorizer\n",
        "\n",
        "# Test sentences\n",
        "test_sentences = [\n",
        "    \"Ti kaneis simera?\", \"To leoforeio argise poly!\", \"Pame gia kafe to apogevma?\",\n",
        "    \"Exo poly douleia kai den prolavaino.\", \"How are you today?\", \"The bus was very late!\",\n",
        "    \"Shall we go for coffee in the afternoon?\", \"I have a lot of work and can't make it.\"\n",
        "]\n",
        "\n",
        "# Transform test data using TF-IDF\n",
        "X_test = vectorizer.transform(test_sentences)\n",
        "\n",
        "# Make predictions\n",
        "predictions = logistic_model.predict(X_test)\n",
        "\n",
        "# Output results\n",
        "for sentence, pred in zip(test_sentences, predictions):\n",
        "    print(f\"'{sentence}' → Predicted as: {'Greeklish' if pred == 1 else 'English'}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqCtb0fKcCup",
        "outputId": "7b16a5bf-3060-43d4-8b18-ba3c25c8601f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Ti kaneis simera?' → Predicted as: English\n",
            "'To leoforeio argise poly!' → Predicted as: English\n",
            "'Pame gia kafe to apogevma?' → Predicted as: English\n",
            "'Exo poly douleia kai den prolavaino.' → Predicted as: English\n",
            "'How are you today?' → Predicted as: English\n",
            "'The bus was very late!' → Predicted as: English\n",
            "'Shall we go for coffee in the afternoon?' → Predicted as: English\n",
            "'I have a lot of work and can't make it.' → Predicted as: English\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import joblib\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the trained LSTM model and tokenizer\n",
        "lstm_model = tf.keras.models.load_model(\"lstm_text_classifier.h5\")\n",
        "tokenizer = joblib.load(\"tokenizer.pkl\")\n",
        "\n",
        "# Test sentences (same as above)\n",
        "test_sentences = [\n",
        "    \"Ti kaneis simera?\", \"To leoforeio argise poly!\", \"Pame gia kafe to apogevma?\",\n",
        "    \"Exo poly douleia kai den prolavaino.\", \"How are you today?\", \"The bus was very late!\",\n",
        "    \"Shall we go for coffee in the afternoon?\", \"I have a lot of work and can't make it.\"\n",
        "]\n",
        "\n",
        "# Tokenize & Pad sequences\n",
        "X_test = tokenizer.texts_to_sequences(test_sentences)\n",
        "X_test = pad_sequences(X_test, maxlen=50, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "# Make predictions\n",
        "predictions = lstm_model.predict(X_test)\n",
        "predicted_labels = [\"Greeklish\" if pred > 0.5 else \"English\" for pred in predictions]\n",
        "\n",
        "# Output results\n",
        "for sentence, pred_label in zip(test_sentences, predicted_labels):\n",
        "    print(f\"'{sentence}' → Predicted as: {pred_label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kw5C2AmhcaPC",
        "outputId": "e9e79b62-fc7a-4cba-f028-ff8cca807451"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 539ms/step\n",
            "'Ti kaneis simera?' → Predicted as: English\n",
            "'To leoforeio argise poly!' → Predicted as: English\n",
            "'Pame gia kafe to apogevma?' → Predicted as: English\n",
            "'Exo poly douleia kai den prolavaino.' → Predicted as: English\n",
            "'How are you today?' → Predicted as: English\n",
            "'The bus was very late!' → Predicted as: English\n",
            "'Shall we go for coffee in the afternoon?' → Predicted as: English\n",
            "'I have a lot of work and can't make it.' → Predicted as: English\n"
          ]
        }
      ]
    }
  ]
}